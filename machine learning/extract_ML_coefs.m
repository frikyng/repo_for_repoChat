%% Extract and display results from a machine elarning ouput
% -------------------------------------------------------------------------
%% Syntax:
%   [all_scores, valid_trace_idx]  = extract_ML_coefs(obj,
%           training_output, rendering, metric, cmap)
%
% -------------------------------------------------------------------------
%% Inputs:
%   obj(arboreal_scan_experiment object, or CHAR) - Optional - Default is
%           empty
%       obj can be :
%       * An arboreal_scan_experiment object (Recommended)
%       * A STR PATH to an extracted arboreal_scan_experiment
%       * an empty value, in whic case it will look for the current folder
%
%   training_output(1xN CELL ARRAY of MACHINE LEARNING OUPTUT STRUCTURE) :
%       * Structure containing one cell per model training, as generated by
%       the predict_behaviour function
%
%   rendering(char) - Optional - Default is 0
%       * If 1, a tree with the selected metrics is display.
%       * If 2, a plot of the model score vs number of ROIs is generated
%
%   metric(CHAR) - Optional - Default is Beta 
%       * One of the valid score metrics.
%           - beta will plot the beta coef from the model
%           - 'explained variance' will display the individual explained
%               variance for each ROI
%           - 'order' will display the ranking of each ROi based on they
%               contribution to the model using the beta coef
%
%   cmap(char) - Optional - Default is 'redblue'
%       The colormap applied to the tree
% -------------------------------------------------------------------------
%% Outputs:
%
%	all_scores (1xN CELL ARRAY of 1xM VALUES) :
%       * For each N behaviour, a 1xM score with M being the number of
%       valid ROIs (see valid_trace_idx to know which ones)
%
%   valid_trace_idx(1 x N INT)
%       The ROIs used for precition
%
% -------------------------------------------------------------------------
%% Extra Notes:
% -------------------------------------------------------------------------
%% Examples:
%
% * Plot explained_variance chart from a previously computed model
%   expe_path = 'C:\Users\Antoine.Valera\MATLAB\newextraction_raw_zscored\2019-10-01_exp_1'
%   % load previous model
%   model_fname = 'Prediction on all behaviours using events modulation on all ROIs.mat'
%   load([expe_path, '\', model_fname]);
%   % load arboreal_experiment_object
%   obj = arboreal_scan_experiment(expe_path);
%   % plot explained variance per ROI
%   extract_ML_coefs(obj, results, true, 'explained_variance');
%   
% -------------------------------------------------------------------------
%% Author(s):
%   Antoine Valera  
% -------------------------------------------------------------------------
%% Revision Date:
%   17-04-2023
% -------------------------------------------------------------------------
%% See also:
%    


function [all_scores, valid_trace_idx]  = extract_ML_coefs(obj, training_output, rendering, metric, cmap)
    if ~isa(obj, 'arboreal_scan_experiment')
        error('Input must be an extarcted arboreal_Scan_experiment object')
    end
    if nargin < 3 || isempty(rendering)
        rendering = true;
    end
    if nargin < 4 || isempty(metric)
        metric = 'beta';
    end
    if nargin < 5 || isempty(cmap)
        cmap = 'redblue';
    end

    %ROIs            = training_output{1}.used_ROIs; % Note that in the case of randomized shuffle, this actually changes from one training to the next
    labels          = fix_labels(training_output{1}.beh_type);
    valid_trace_idx = training_output{1}.used_ROIs;    
    behaviours      = training_output{1}.beh_type;
    all_scores      = {};
    not_shuffled_idx= find(~contains(training_output{1}.beh_type, '_shuffled'));
    
    %% If you use explained variance, we need to rextract the predictors and observations if they were not saved.
    if strcmpi(metric, 'explained_variance')
        obj.beh_smoothing               = [-1.5, 0];
        obj.bad_ROI_thr                 = 0;
        obj.find_events();
        obj.rescale_traces();
        tp                              = training_output{1}.timepoints{1};
        predictors                      = obj.rescaled_traces(tp,:) - obj.global_median_rescaled(tp);
        behaviours                      = training_output{1}.beh_type(not_shuffled_idx);
        [~, raw_behaviours, ~, names]   = prepare_behaviour_data(obj, behaviours);
        raw_behaviours                  = normalize(raw_behaviours');
        raw_behaviours                  = raw_behaviours(tp,sort(repmat(1:7,1,2)));
        close all
        %ROI_list = find(~ismember(obj.ref.indices.valid_swc_rois, obj.bad_ROI_list));
    end
    ROI_list                        = cell2mat(training_output{1}.used_ROIs); % List of ROis used during training
    
    %% Now, get value for each type of observation
    for beh_idx = not_shuffled_idx
        all_scores{beh_idx}   = [];        
        for iter = 1:numel(training_output)            
            model           = training_output{iter}.model{beh_idx};
            if isprop(model, 'Trained')
                for trained_idx = 1:numel(model.Trained)
                    mdl = model.Trained{trained_idx}; 
                    if strcmpi(metric, 'beta')
                        all_scores{beh_idx}(trained_idx, :) = mdl.Beta;
                    elseif strcmpi(metric, 'order')
                        [~, all_scores{beh_idx}(trained_idx, :)] = sort(abs(model.Beta));  
                    end
                end
            else
                if strcmpi(metric, 'beta')
                    all_scores{beh_idx}(iter, :)            = model.Beta;     
                elseif strcmpi(metric, 'order')
                    [~, all_scores{beh_idx}(iter, :)]       = sort(abs(model.Beta));    
                elseif strcmpi(metric, 'explained_variance')
                    predictions                             = model.Bias + model.Beta .* predictors(:,ROI_list)';
                    for roi = 1:numel(ROI_list)
                        all_scores{beh_idx}(iter, roi)      = explained_variance_score(predictions(roi, :), raw_behaviours(:, beh_idx), '');
                    end
                    [~, order]          = sort(abs(model.Beta), 'descend');
                    beta_sorted         = model.Beta(order);
                    ROI_list_sorted     = ROI_list(order);
                    n_coefs             = 1:numel(model.Beta);
                    for coef = n_coefs
                        current_pred = model.Bias;
                        for i = 1:coef
                            current_pred = current_pred + beta_sorted(i) * predictors(:,ROI_list_sorted(i));
                        end  
                        expl_var(iter, coef) = explained_variance_score(current_pred, raw_behaviours(:, beh_idx), '');                        
                    end
                end
            end
        end
        
        %% Plot learning performance
        if rendering && strcmpi(metric, 'explained_variance')
            %close all
            figure();plot(n_coefs, expl_var, 'Color', [0.8,0.8,0.8]); hold on
            plot(n_coefs, nanmean(expl_var, 1),'ko-','MarkerFaceColor', 'k'); hold on;ylim([0,75])   
            set(gca,'box','off');hold on;xlabel('number of ROI');ylabel('Explained Variance');set(gcf, 'Color', 'w');title(['Explained variance for ',labels{beh_idx},' with increasing number of ROIs']);
            all_scores{beh_idx} = expl_var;
        end
                
        if rendering
            next_fig
            obj.ref.plot_value_tree(mean(all_scores{beh_idx},1), ROI_list,'',labels{beh_idx},'',beh_idx,'',cmap,'invalid_color');
            if strcmpi(metric, 'explained_variance')
                caxis([0,max(nanmean(expl_var,1))]);
            end
        end
    end
    
    
%     close all
%     ref = nanmean(cat(1, all_scores{:}));
%     for beh_idx = not_shuffled_idx
%         new = nanmean(all_scores{beh_idx}, 1);
%         new = new / (new / ref);
%         new = new - ref;
%         next_fig
%         obj.ref.plot_value_tree(new, ROI_list,'',labels{beh_idx},'',beh_idx,'','redblue','invalid_color');
%         if strcmpi(metric, 'explained_variance')
%             caxis([-5, 5]);
%         end
%     end
    
    
    
    
end

